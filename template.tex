\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\usepackage{amsthm}
 
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[section]

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Abstract Differentiable Memory}


\author{
  Henry Steinitz
  \thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
    New York University, Google\\
    New York, NY 15213 \\
  \texttt{henrysteinitz@google.com} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

Contemporary AI systems are composed of differentiable modules designed for specific tasks, such as perception, attention, and memory. They are often trained end-to-end by stochastic gradient descent, and each module is rarely tested individually at the task it was designed to perform.

\section{Sequence Memory}

We begin with a simple model of long-term memory, where the goal is to remember a sequence of points $x_1, \dots x_n$ from an input space $X$ by iteratively updating a running representation $m_i$ in a memory space $M$. Then, we'd like reverse this process to recover previous $x_i$ from $m_n$. Formally,

\begin{definition}
Let $X$ be a Riemannian manifold called the input space. A sequence memory $(M, \pi, \gamma)$ on $X$ is a Riemannian manifold $M$ with two smooth maps:
\begin{itemize}
    \item $\pi: M \times X \rightarrow M$ called the save map.
    \item $\gamma: M \times [0, \infty) \rightarrow X$ called the recovery map.
\end{itemize}
\end{definition} 

We use the closed interval $[0, \infty)$ because we want the recovery map to be differentiable. but any map $\gamma: M \times \bbZ_{\geq 0} \rightarrow X$ can be extended to map on $M \times [0, \infty)$ by interpolation. An alternative is to define the recovery map as $\gamma: M \rightarrow X \times M$ so that applying gamma returns the previously seen $x_i$ and a previous version on the memory $mi$. However, this would mean the complexity of $\gamma^t$ would be at least linear, and we ultimately want recovery to be sublinear in $t$.
\par
When the save and recovery maps are clear from context, we'll abbreviate $(M, \pi, \gamma)$ as $M$. It's likely useful to study sequence memory and other constructions in this work in a more general setting. For example, one may take $X$ and $M$ to be metric spaces or topological spaces. However, we're specifically interested in studying memory in differentiable systems, so we leave this for future work.
\par
Our definition induces the following iterated save maps
\[
\pi^n(M, x_1, \dots, x_n) = \pi(\pi^{n-1}(M, x_1, \dots, x_{n-1}), x_n),
\]
with $\pi^1 = \pi$. Similarly we can define the iterated recovery maps
\[
\gamma^n(M
\]

\subsection{Perfect sequence memory must be infinite dimensional}

\begin{definition}
A sequence memory $(M, \pi, \gamma)$ on $X$ is \textit{perfect} if for all sequences $x_1, \dots x_n$,
\[
\gamma(\pi^n(M, x_1, \dots, x_n), t) = x_t
\]
for all $t \in \bbZ_{>0}$
\end{definition}
 
The following simple result 
\begin{theorem}
Let $X$ be an input space with dimension $> 0$. Then if $M$ is a perfect memory on $X$, $M$ is infinite dimensional.
\end{theorem}

\begin{proof}
For any sequence $x_1, \dots, x_n$
\end{proof}

\subsection{Evaluating sequence memory}

\begin{definition}
A sequence memory $(M, \pi, \gamma)$ is $k$\textit{-consistent} if 
\[
\norm{\gamma(M, t) - \gamma(\pi(M, x}, t+1) < O(t^k)
\]
\end{definition}

\begin{theorem}
If $M$ is k consistent, then
\[
\norm{\gamma(\pi^t(M, x_1, \dots, x_t), t - 1) - x_1} < O(t^{k+1})
\]
\end{theorem}

\begin{proof}
\begin{align*}
\norm{\gamma(\pi^t(M, x_1, \dots, x_t), t - 1) - x_1} &=
\norm{\sum_{k = 2}^t \gamma(\pi^k(M, x_1, \dots, x_k), k - 1) - \gamma(\pi^k-2(M, x_1, \dots, x_{k-2}), k - 2)}
\end{align*}
\end{proof}

\section{Associative Memory}




%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}


\end{document}
